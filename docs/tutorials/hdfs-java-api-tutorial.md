# HDFS Java API æ“ä½œæ•™ç¨‹

## ç›®å½•
1. [HDFS Java API æ¦‚è¿°](#1-hdfs-java-api-æ¦‚è¿°)
2. [ç¯å¢ƒå‡†å¤‡](#2-ç¯å¢ƒå‡†å¤‡)
3. [Mavené¡¹ç›®é…ç½®](#3-mavené¡¹ç›®é…ç½®)
4. [æ ¸å¿ƒAPIä»‹ç»](#4-æ ¸å¿ƒapiä»‹ç»)
5. [åŸºç¡€æ“ä½œç¤ºä¾‹](#5-åŸºç¡€æ“ä½œç¤ºä¾‹)
6. [é«˜çº§æ“ä½œç¤ºä¾‹](#6-é«˜çº§æ“ä½œç¤ºä¾‹)
7. [å®Œæ•´é¡¹ç›®æ¡ˆä¾‹](#7-å®Œæ•´é¡¹ç›®æ¡ˆä¾‹)
8. [å¸¸è§é—®é¢˜è§£å†³](#8-å¸¸è§é—®é¢˜è§£å†³)

## 1. HDFS Java API æ¦‚è¿°

### 1.1 ä»€ä¹ˆæ˜¯HDFS Java API
HDFS Java APIæ˜¯Hadoopæä¾›çš„ç”¨äºä¸HDFSæ–‡ä»¶ç³»ç»Ÿè¿›è¡Œäº¤äº’çš„Javaç¼–ç¨‹æ¥å£ã€‚é€šè¿‡è¿™äº›APIï¼Œæˆ‘ä»¬å¯ä»¥ï¼š
- åˆ›å»ºã€åˆ é™¤ã€é‡å‘½åæ–‡ä»¶å’Œç›®å½•
- ä¸Šä¼ å’Œä¸‹è½½æ–‡ä»¶
- è¯»å–å’Œå†™å…¥æ–‡ä»¶å†…å®¹
- è·å–æ–‡ä»¶å’Œç›®å½•ä¿¡æ¯
- è®¾ç½®æ–‡ä»¶æƒé™

### 1.2 æ ¸å¿ƒç±»ä»‹ç»
- **Configuration**: é…ç½®ç±»ï¼Œç”¨äºè®¾ç½®Hadoopé…ç½®å‚æ•°
- **FileSystem**: æ–‡ä»¶ç³»ç»ŸæŠ½è±¡ç±»ï¼Œæä¾›æ–‡ä»¶æ“ä½œæ–¹æ³•
- **Path**: è·¯å¾„ç±»ï¼Œè¡¨ç¤ºHDFSä¸­çš„æ–‡ä»¶æˆ–ç›®å½•è·¯å¾„
- **FSDataInputStream**: HDFSè¾“å…¥æµ
- **FSDataOutputStream**: HDFSè¾“å‡ºæµ

## 2. ç¯å¢ƒå‡†å¤‡

### 2.1 å‰ç½®æ¡ä»¶
- JDK 1.8+
- Maven 3.5+
- Hadoop 3.3.4ï¼ˆå·²å®‰è£…å¹¶å¯åŠ¨ï¼‰
- IntelliJ IDEA æˆ– Eclipse

### 2.2 éªŒè¯Hadoopç¯å¢ƒ
```bash
# æ£€æŸ¥HDFSæ˜¯å¦æ­£å¸¸è¿è¡Œ
hdfs dfsadmin -report

# æ£€æŸ¥Web UI
# è®¿é—® http://localhost:9870
```

## 3. Mavené¡¹ç›®é…ç½®

### 3.1 åˆ›å»ºMavené¡¹ç›®
```bash
# åˆ›å»ºé¡¹ç›®ç›®å½•
mkdir hdfs-java-demo
cd hdfs-java-demo

# åˆ›å»ºMavené¡¹ç›®ç»“æ„
mkdir -p src/main/java/com/bigdata/hdfs
mkdir -p src/main/resources
mkdir -p src/test/java
```

### 3.2 pom.xmlé…ç½®
```xml
<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 
         http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>
    
    <groupId>com.bigdata</groupId>
    <artifactId>hdfs-java-demo</artifactId>
    <version>1.0.0</version>
    <packaging>jar</packaging>
    
    <properties>
        <maven.compiler.source>8</maven.compiler.source>
        <maven.compiler.target>8</maven.compiler.target>
        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
        <hadoop.version>3.3.4</hadoop.version>
        <junit.version>4.13.2</junit.version>
        <slf4j.version>1.7.36</slf4j.version>
    </properties>
    
    <dependencies>
        <!-- Hadoop Client -->
        <dependency>
            <groupId>org.apache.hadoop</groupId>
            <artifactId>hadoop-client</artifactId>
            <version>${hadoop.version}</version>
        </dependency>
        
        <!-- Hadoop HDFS -->
        <dependency>
            <groupId>org.apache.hadoop</groupId>
            <artifactId>hadoop-hdfs</artifactId>
            <version>${hadoop.version}</version>
        </dependency>
        
        <!-- Hadoop Common -->
        <dependency>
            <groupId>org.apache.hadoop</groupId>
            <artifactId>hadoop-common</artifactId>
            <version>${hadoop.version}</version>
        </dependency>
        
        <!-- æ—¥å¿—ä¾èµ– -->
        <dependency>
            <groupId>org.slf4j</groupId>
            <artifactId>slf4j-api</artifactId>
            <version>${slf4j.version}</version>
        </dependency>
        
        <dependency>
            <groupId>org.slf4j</groupId>
            <artifactId>slf4j-log4j12</artifactId>
            <version>${slf4j.version}</version>
        </dependency>
        
        <!-- æµ‹è¯•ä¾èµ– -->
        <dependency>
            <groupId>junit</groupId>
            <artifactId>junit</artifactId>
            <version>${junit.version}</version>
            <scope>test</scope>
        </dependency>
    </dependencies>
    
    <build>
        <plugins>
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-compiler-plugin</artifactId>
                <version>3.8.1</version>
                <configuration>
                    <source>8</source>
                    <target>8</target>
                </configuration>
            </plugin>
        </plugins>
    </build>
</project>
```

### 3.3 log4j.propertiesé…ç½®
åœ¨ `src/main/resources` ç›®å½•ä¸‹åˆ›å»º `log4j.properties`ï¼š
```properties
log4j.rootLogger=INFO, stdout
log4j.appender.stdout=org.apache.log4j.ConsoleAppender
log4j.appender.stdout.layout=org.apache.log4j.PatternLayout
log4j.appender.stdout.layout.ConversionPattern=%d{yyyy-MM-dd HH:mm:ss} %-5p %c{1}:%L - %m%n

# å‡å°‘Hadoopæ—¥å¿—è¾“å‡º
log4j.logger.org.apache.hadoop=WARN
log4j.logger.org.apache.zookeeper=WARN
```

## 4. æ ¸å¿ƒAPIä»‹ç»

### 4.1 Configurationç±»
```java
// åˆ›å»ºé…ç½®å¯¹è±¡
Configuration conf = new Configuration();

// è®¾ç½®HDFSåœ°å€
conf.set("fs.defaultFS", "hdfs://10.132.144.24:9000");

// è®¾ç½®ç”¨æˆ·åï¼ˆå¯é€‰ï¼‰
System.setProperty("HADOOP_USER_NAME", "hadoop");
```

### 4.2 FileSystemç±»
```java
// è·å–æ–‡ä»¶ç³»ç»Ÿå®ä¾‹
FileSystem fs = FileSystem.get(conf);

// æˆ–è€…æŒ‡å®šURI
URI uri = new URI("hdfs://10.132.144.24:9000");
FileSystem fs = FileSystem.get(uri, conf);
```

### 4.3 Pathç±»
```java
// åˆ›å»ºè·¯å¾„å¯¹è±¡
Path path = new Path("/user/data/test.txt");

// è·¯å¾„æ“ä½œ
Path parent = path.getParent();  // è·å–çˆ¶ç›®å½•
String name = path.getName();    // è·å–æ–‡ä»¶å
```

## 5. åŸºç¡€æ“ä½œç¤ºä¾‹

### 5.1 HDFSå·¥å…·ç±»
```java
package com.bigdata.hdfs.util;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.*;
import org.apache.hadoop.io.IOUtils;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.io.*;
import java.net.URI;

/**
 * HDFSæ“ä½œå·¥å…·ç±»
 * æä¾›HDFSæ–‡ä»¶ç³»ç»Ÿçš„åŸºæœ¬æ“ä½œåŠŸèƒ½
 * 
 * @author BigData Team
 * @version 1.0.0
 */
public class HDFSUtil {
    
    private static final Logger logger = LoggerFactory.getLogger(HDFSUtil.class);
    
    private FileSystem fileSystem;
    private Configuration configuration;
    
    /**
     * æ„é€ å‡½æ•°ï¼Œåˆå§‹åŒ–HDFSè¿æ¥
     * 
     * @param hdfsUri HDFSåœ°å€ï¼Œä¾‹å¦‚ï¼šhdfs://10.132.144.24:9000
     * @throws Exception åˆå§‹åŒ–å¼‚å¸¸
     */
    public HDFSUtil(String hdfsUri) throws Exception {
        this.configuration = new Configuration();
        this.configuration.set("fs.defaultFS", hdfsUri);
        
        // è®¾ç½®ç”¨æˆ·åï¼Œé¿å…æƒé™é—®é¢˜
        System.setProperty("HADOOP_USER_NAME", "hadoop");
        
        // è·å–æ–‡ä»¶ç³»ç»Ÿå®ä¾‹
        this.fileSystem = FileSystem.get(URI.create(hdfsUri), configuration);
        
        logger.info("HDFSè¿æ¥åˆå§‹åŒ–æˆåŠŸ: {}", hdfsUri);
    }
    
    /**
     * åˆ›å»ºç›®å½•
     * 
     * @param dirPath ç›®å½•è·¯å¾„
     * @return åˆ›å»ºæˆåŠŸè¿”å›trueï¼Œå¦åˆ™è¿”å›false
     */
    public boolean createDirectory(String dirPath) {
        try {
            Path path = new Path(dirPath);
            if (fileSystem.exists(path)) {
                logger.warn("ç›®å½•å·²å­˜åœ¨: {}", dirPath);
                return true;
            }
            
            boolean result = fileSystem.mkdirs(path);
            if (result) {
                logger.info("ç›®å½•åˆ›å»ºæˆåŠŸ: {}", dirPath);
            } else {
                logger.error("ç›®å½•åˆ›å»ºå¤±è´¥: {}", dirPath);
            }
            return result;
        } catch (Exception e) {
            logger.error("åˆ›å»ºç›®å½•å¼‚å¸¸: {}", dirPath, e);
            return false;
        }
    }
    
    /**
     * ä¸Šä¼ æœ¬åœ°æ–‡ä»¶åˆ°HDFS
     * 
     * @param localFilePath æœ¬åœ°æ–‡ä»¶è·¯å¾„
     * @param hdfsFilePath HDFSæ–‡ä»¶è·¯å¾„
     * @return ä¸Šä¼ æˆåŠŸè¿”å›trueï¼Œå¦åˆ™è¿”å›false
     */
    public boolean uploadFile(String localFilePath, String hdfsFilePath) {
        try {
            Path localPath = new Path(localFilePath);
            Path hdfsPath = new Path(hdfsFilePath);
            
            // æ£€æŸ¥æœ¬åœ°æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            File localFile = new File(localFilePath);
            if (!localFile.exists()) {
                logger.error("æœ¬åœ°æ–‡ä»¶ä¸å­˜åœ¨: {}", localFilePath);
                return false;
            }
            
            // å¦‚æœHDFSæ–‡ä»¶å·²å­˜åœ¨ï¼Œå…ˆåˆ é™¤
            if (fileSystem.exists(hdfsPath)) {
                fileSystem.delete(hdfsPath, false);
                logger.info("åˆ é™¤å·²å­˜åœ¨çš„HDFSæ–‡ä»¶: {}", hdfsFilePath);
            }
            
            // ä¸Šä¼ æ–‡ä»¶
            fileSystem.copyFromLocalFile(localPath, hdfsPath);
            logger.info("æ–‡ä»¶ä¸Šä¼ æˆåŠŸ: {} -> {}", localFilePath, hdfsFilePath);
            return true;
        } catch (Exception e) {
            logger.error("æ–‡ä»¶ä¸Šä¼ å¼‚å¸¸: {} -> {}", localFilePath, hdfsFilePath, e);
            return false;
        }
    }
    
    /**
     * ä»HDFSä¸‹è½½æ–‡ä»¶åˆ°æœ¬åœ°
     * 
     * @param hdfsFilePath HDFSæ–‡ä»¶è·¯å¾„
     * @param localFilePath æœ¬åœ°æ–‡ä»¶è·¯å¾„
     * @return ä¸‹è½½æˆåŠŸè¿”å›trueï¼Œå¦åˆ™è¿”å›false
     */
    public boolean downloadFile(String hdfsFilePath, String localFilePath) {
        try {
            Path hdfsPath = new Path(hdfsFilePath);
            Path localPath = new Path(localFilePath);
            
            // æ£€æŸ¥HDFSæ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if (!fileSystem.exists(hdfsPath)) {
                logger.error("HDFSæ–‡ä»¶ä¸å­˜åœ¨: {}", hdfsFilePath);
                return false;
            }
            
            // ä¸‹è½½æ–‡ä»¶
            fileSystem.copyToLocalFile(hdfsPath, localPath);
            logger.info("æ–‡ä»¶ä¸‹è½½æˆåŠŸ: {} -> {}", hdfsFilePath, localFilePath);
            return true;
        } catch (Exception e) {
            logger.error("æ–‡ä»¶ä¸‹è½½å¼‚å¸¸: {} -> {}", hdfsFilePath, localFilePath, e);
            return false;
        }
    }
    
    /**
     * åˆ é™¤HDFSæ–‡ä»¶æˆ–ç›®å½•
     * 
     * @param hdfsPath HDFSè·¯å¾„
     * @param recursive æ˜¯å¦é€’å½’åˆ é™¤ï¼ˆç”¨äºç›®å½•ï¼‰
     * @return åˆ é™¤æˆåŠŸè¿”å›trueï¼Œå¦åˆ™è¿”å›false
     */
    public boolean deleteFile(String hdfsPath, boolean recursive) {
        try {
            Path path = new Path(hdfsPath);
            
            if (!fileSystem.exists(path)) {
                logger.warn("æ–‡ä»¶æˆ–ç›®å½•ä¸å­˜åœ¨: {}", hdfsPath);
                return true;
            }
            
            boolean result = fileSystem.delete(path, recursive);
            if (result) {
                logger.info("åˆ é™¤æˆåŠŸ: {}", hdfsPath);
            } else {
                logger.error("åˆ é™¤å¤±è´¥: {}", hdfsPath);
            }
            return result;
        } catch (Exception e) {
            logger.error("åˆ é™¤å¼‚å¸¸: {}", hdfsPath, e);
            return false;
        }
    }
    
    /**
     * åˆ—å‡ºç›®å½•ä¸‹çš„æ–‡ä»¶å’Œå­ç›®å½•
     * 
     * @param dirPath ç›®å½•è·¯å¾„
     * @return æ–‡ä»¶çŠ¶æ€æ•°ç»„
     */
    public FileStatus[] listFiles(String dirPath) {
        try {
            Path path = new Path(dirPath);
            
            if (!fileSystem.exists(path)) {
                logger.error("ç›®å½•ä¸å­˜åœ¨: {}", dirPath);
                return new FileStatus[0];
            }
            
            FileStatus[] fileStatuses = fileSystem.listStatus(path);
            logger.info("ç›®å½• {} åŒ…å« {} ä¸ªæ–‡ä»¶/ç›®å½•", dirPath, fileStatuses.length);
            
            for (FileStatus status : fileStatuses) {
                String type = status.isDirectory() ? "ç›®å½•" : "æ–‡ä»¶";
                logger.info("{}: {} (å¤§å°: {} å­—èŠ‚)", type, status.getPath().getName(), status.getLen());
            }
            
            return fileStatuses;
        } catch (Exception e) {
            logger.error("åˆ—å‡ºæ–‡ä»¶å¼‚å¸¸: {}", dirPath, e);
            return new FileStatus[0];
        }
    }
    
    /**
     * è¯»å–HDFSæ–‡ä»¶å†…å®¹
     * 
     * @param hdfsFilePath HDFSæ–‡ä»¶è·¯å¾„
     * @return æ–‡ä»¶å†…å®¹å­—ç¬¦ä¸²
     */
    public String readFile(String hdfsFilePath) {
        FSDataInputStream inputStream = null;
        try {
            Path path = new Path(hdfsFilePath);
            
            if (!fileSystem.exists(path)) {
                logger.error("æ–‡ä»¶ä¸å­˜åœ¨: {}", hdfsFilePath);
                return null;
            }
            
            inputStream = fileSystem.open(path);
            ByteArrayOutputStream outputStream = new ByteArrayOutputStream();
            IOUtils.copyBytes(inputStream, outputStream, configuration);
            
            String content = outputStream.toString("UTF-8");
            logger.info("æ–‡ä»¶è¯»å–æˆåŠŸ: {} (å¤§å°: {} å­—èŠ‚)", hdfsFilePath, content.length());
            return content;
        } catch (Exception e) {
            logger.error("æ–‡ä»¶è¯»å–å¼‚å¸¸: {}", hdfsFilePath, e);
            return null;
        } finally {
            IOUtils.closeStream(inputStream);
        }
    }
    
    /**
     * å†™å…¥å†…å®¹åˆ°HDFSæ–‡ä»¶
     * 
     * @param hdfsFilePath HDFSæ–‡ä»¶è·¯å¾„
     * @param content æ–‡ä»¶å†…å®¹
     * @return å†™å…¥æˆåŠŸè¿”å›trueï¼Œå¦åˆ™è¿”å›false
     */
    public boolean writeFile(String hdfsFilePath, String content) {
        FSDataOutputStream outputStream = null;
        try {
            Path path = new Path(hdfsFilePath);
            
            // å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œå…ˆåˆ é™¤
            if (fileSystem.exists(path)) {
                fileSystem.delete(path, false);
            }
            
            outputStream = fileSystem.create(path);
            outputStream.write(content.getBytes("UTF-8"));
            outputStream.flush();
            
            logger.info("æ–‡ä»¶å†™å…¥æˆåŠŸ: {} (å¤§å°: {} å­—èŠ‚)", hdfsFilePath, content.length());
            return true;
        } catch (Exception e) {
            logger.error("æ–‡ä»¶å†™å…¥å¼‚å¸¸: {}", hdfsFilePath, e);
            return false;
        } finally {
            IOUtils.closeStream(outputStream);
        }
    }
    
    /**
     * è·å–æ–‡ä»¶ä¿¡æ¯
     * 
     * @param hdfsFilePath HDFSæ–‡ä»¶è·¯å¾„
     * @return æ–‡ä»¶çŠ¶æ€å¯¹è±¡
     */
    public FileStatus getFileStatus(String hdfsFilePath) {
        try {
            Path path = new Path(hdfsFilePath);
            
            if (!fileSystem.exists(path)) {
                logger.error("æ–‡ä»¶ä¸å­˜åœ¨: {}", hdfsFilePath);
                return null;
            }
            
            FileStatus status = fileSystem.getFileStatus(path);
            logger.info("æ–‡ä»¶ä¿¡æ¯ - è·¯å¾„: {}, å¤§å°: {} å­—èŠ‚, ä¿®æ”¹æ—¶é—´: {}", 
                       status.getPath(), status.getLen(), status.getModificationTime());
            return status;
        } catch (Exception e) {
            logger.error("è·å–æ–‡ä»¶ä¿¡æ¯å¼‚å¸¸: {}", hdfsFilePath, e);
            return null;
        }
    }
    
    /**
     * å…³é—­æ–‡ä»¶ç³»ç»Ÿè¿æ¥
     */
    public void close() {
        try {
            if (fileSystem != null) {
                fileSystem.close();
                logger.info("HDFSè¿æ¥å·²å…³é—­");
            }
        } catch (Exception e) {
            logger.error("å…³é—­HDFSè¿æ¥å¼‚å¸¸", e);
        }
    }
}
```

### 5.2 åŸºç¡€æ“ä½œç¤ºä¾‹ç±»
```java
package com.bigdata.hdfs.example;

import com.bigdata.hdfs.util.HDFSUtil;
import org.apache.hadoop.fs.FileStatus;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * HDFSåŸºç¡€æ“ä½œç¤ºä¾‹
 * æ¼”ç¤ºHDFSçš„åŸºæœ¬æ–‡ä»¶æ“ä½œåŠŸèƒ½
 * 
 * @author BigData Team
 * @version 1.0.0
 */
public class HDFSBasicExample {
    
    private static final Logger logger = LoggerFactory.getLogger(HDFSBasicExample.class);
    
    public static void main(String[] args) {
        HDFSUtil hdfsUtil = null;
        
        try {
            // åˆå§‹åŒ–HDFSè¿æ¥
            hdfsUtil = new HDFSUtil("hdfs://10.132.144.24:9000");
            
            // 1. åˆ›å»ºç›®å½•
            logger.info("=== 1. åˆ›å»ºç›®å½•æµ‹è¯• ===");
            hdfsUtil.createDirectory("/user/bigdata");
            hdfsUtil.createDirectory("/user/bigdata/input");
            hdfsUtil.createDirectory("/user/bigdata/output");
            
            // 2. å†™å…¥æ–‡ä»¶
            logger.info("=== 2. å†™å…¥æ–‡ä»¶æµ‹è¯• ===");
            String content = "Hello HDFS!\nè¿™æ˜¯ä¸€ä¸ªHDFS Java APIæµ‹è¯•æ–‡ä»¶ã€‚\nå½“å‰æ—¶é—´: " + 
                           new java.util.Date();
            hdfsUtil.writeFile("/user/bigdata/test.txt", content);
            
            // 3. è¯»å–æ–‡ä»¶
            logger.info("=== 3. è¯»å–æ–‡ä»¶æµ‹è¯• ===");
            String readContent = hdfsUtil.readFile("/user/bigdata/test.txt");
            logger.info("è¯»å–çš„æ–‡ä»¶å†…å®¹:\n{}", readContent);
            
            // 4. è·å–æ–‡ä»¶ä¿¡æ¯
            logger.info("=== 4. è·å–æ–‡ä»¶ä¿¡æ¯æµ‹è¯• ===");
            FileStatus fileStatus = hdfsUtil.getFileStatus("/user/bigdata/test.txt");
            if (fileStatus != null) {
                logger.info("æ–‡ä»¶è·¯å¾„: {}", fileStatus.getPath());
                logger.info("æ–‡ä»¶å¤§å°: {} å­—èŠ‚", fileStatus.getLen());
                logger.info("æ˜¯å¦ä¸ºç›®å½•: {}", fileStatus.isDirectory());
                logger.info("ä¿®æ”¹æ—¶é—´: {}", new java.util.Date(fileStatus.getModificationTime()));
                logger.info("æ–‡ä»¶æƒé™: {}", fileStatus.getPermission());
            }
            
            // 5. åˆ—å‡ºç›®å½•å†…å®¹
            logger.info("=== 5. åˆ—å‡ºç›®å½•å†…å®¹æµ‹è¯• ===");
            hdfsUtil.listFiles("/user/bigdata");
            
            // 6. åˆ›å»ºæœ¬åœ°æµ‹è¯•æ–‡ä»¶å¹¶ä¸Šä¼ 
            logger.info("=== 6. æ–‡ä»¶ä¸Šä¼ æµ‹è¯• ===");
            createLocalTestFile();
            hdfsUtil.uploadFile("local_test.txt", "/user/bigdata/uploaded_test.txt");
            
            // 7. ä¸‹è½½æ–‡ä»¶
            logger.info("=== 7. æ–‡ä»¶ä¸‹è½½æµ‹è¯• ===");
            hdfsUtil.downloadFile("/user/bigdata/uploaded_test.txt", "downloaded_test.txt");
            
            // 8. åˆ é™¤æ–‡ä»¶
            logger.info("=== 8. åˆ é™¤æ–‡ä»¶æµ‹è¯• ===");
            hdfsUtil.deleteFile("/user/bigdata/test.txt", false);
            
            logger.info("=== HDFSåŸºç¡€æ“ä½œæµ‹è¯•å®Œæˆ ===");
            
        } catch (Exception e) {
            logger.error("HDFSæ“ä½œå¼‚å¸¸", e);
        } finally {
            // å…³é—­è¿æ¥
            if (hdfsUtil != null) {
                hdfsUtil.close();
            }
        }
    }
    
    /**
     * åˆ›å»ºæœ¬åœ°æµ‹è¯•æ–‡ä»¶
     */
    private static void createLocalTestFile() {
        try {
            java.io.FileWriter writer = new java.io.FileWriter("local_test.txt");
            writer.write("è¿™æ˜¯ä¸€ä¸ªæœ¬åœ°æµ‹è¯•æ–‡ä»¶\n");
            writer.write("ç”¨äºæµ‹è¯•HDFSæ–‡ä»¶ä¸Šä¼ åŠŸèƒ½\n");
            writer.write("åˆ›å»ºæ—¶é—´: " + new java.util.Date() + "\n");
            writer.close();
            logger.info("æœ¬åœ°æµ‹è¯•æ–‡ä»¶åˆ›å»ºæˆåŠŸ: local_test.txt");
        } catch (Exception e) {
            logger.error("åˆ›å»ºæœ¬åœ°æµ‹è¯•æ–‡ä»¶å¤±è´¥", e);
        }
    }
}
```

## 6. é«˜çº§æ“ä½œç¤ºä¾‹

### 6.1 å¤§æ–‡ä»¶å¤„ç†ç¤ºä¾‹
```java
package com.bigdata.hdfs.example;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.*;
import org.apache.hadoop.io.IOUtils;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.io.BufferedReader;
import java.io.InputStreamReader;
import java.net.URI;

/**
 * HDFSå¤§æ–‡ä»¶å¤„ç†ç¤ºä¾‹
 * æ¼”ç¤ºå¦‚ä½•å¤„ç†å¤§æ–‡ä»¶çš„è¯»å†™æ“ä½œ
 * 
 * @author BigData Team
 * @version 1.0.0
 */
public class HDFSLargeFileExample {
    
    private static final Logger logger = LoggerFactory.getLogger(HDFSLargeFileExample.class);
    
    public static void main(String[] args) {
        try {
            Configuration conf = new Configuration();
            conf.set("fs.defaultFS", "hdfs://10.132.144.24:9000");
            System.setProperty("HADOOP_USER_NAME", "hadoop");
            
            FileSystem fs = FileSystem.get(URI.create("hdfs://10.132.144.24:9000"), conf);
            
            // 1. åˆ›å»ºå¤§æ–‡ä»¶
            createLargeFile(fs);
            
            // 2. æµå¼è¯»å–å¤§æ–‡ä»¶
            streamReadLargeFile(fs);
            
            // 3. åˆ†å—è¯»å–å¤§æ–‡ä»¶
            blockReadLargeFile(fs);
            
            fs.close();
            
        } catch (Exception e) {
            logger.error("å¤§æ–‡ä»¶å¤„ç†å¼‚å¸¸", e);
        }
    }
    
    /**
     * åˆ›å»ºå¤§æ–‡ä»¶ï¼ˆæ¨¡æ‹Ÿï¼‰
     * 
     * @param fs æ–‡ä»¶ç³»ç»Ÿå¯¹è±¡
     */
    private static void createLargeFile(FileSystem fs) {
        FSDataOutputStream out = null;
        try {
            Path largePath = new Path("/user/bigdata/large_file.txt");
            
            // å¦‚æœæ–‡ä»¶å­˜åœ¨ï¼Œå…ˆåˆ é™¤
            if (fs.exists(largePath)) {
                fs.delete(largePath, false);
            }
            
            out = fs.create(largePath);
            
            // å†™å…¥å¤§é‡æ•°æ®ï¼ˆæ¨¡æ‹Ÿå¤§æ–‡ä»¶ï¼‰
            for (int i = 0; i < 100000; i++) {
                String line = String.format("è¿™æ˜¯ç¬¬ %d è¡Œæ•°æ®ï¼ŒåŒ…å«ä¸€äº›æµ‹è¯•å†…å®¹ç”¨äºæ¼”ç¤ºå¤§æ–‡ä»¶å¤„ç†\n", i + 1);
                out.write(line.getBytes("UTF-8"));
                
                if (i % 10000 == 0) {
                    logger.info("å·²å†™å…¥ {} è¡Œæ•°æ®", i);
                }
            }
            
            out.flush();
            logger.info("å¤§æ–‡ä»¶åˆ›å»ºå®Œæˆ: {}", largePath);
            
        } catch (Exception e) {
            logger.error("åˆ›å»ºå¤§æ–‡ä»¶å¼‚å¸¸", e);
        } finally {
            IOUtils.closeStream(out);
        }
    }
    
    /**
     * æµå¼è¯»å–å¤§æ–‡ä»¶
     * 
     * @param fs æ–‡ä»¶ç³»ç»Ÿå¯¹è±¡
     */
    private static void streamReadLargeFile(FileSystem fs) {
        FSDataInputStream in = null;
        BufferedReader reader = null;
        
        try {
            Path largePath = new Path("/user/bigdata/large_file.txt");
            
            if (!fs.exists(largePath)) {
                logger.error("å¤§æ–‡ä»¶ä¸å­˜åœ¨: {}", largePath);
                return;
            }
            
            in = fs.open(largePath);
            reader = new BufferedReader(new InputStreamReader(in, "UTF-8"));
            
            String line;
            int lineCount = 0;
            long startTime = System.currentTimeMillis();
            
            while ((line = reader.readLine()) != null) {
                lineCount++;
                
                // æ¯è¯»å–1000è¡Œè¾“å‡ºä¸€æ¬¡è¿›åº¦
                if (lineCount % 1000 == 0) {
                    logger.info("å·²è¯»å– {} è¡Œ", lineCount);
                }
                
                // è¿™é‡Œå¯ä»¥å¤„ç†æ¯ä¸€è¡Œæ•°æ®
                // processLine(line);
            }
            
            long endTime = System.currentTimeMillis();
            logger.info("æµå¼è¯»å–å®Œæˆï¼Œæ€»å…±è¯»å– {} è¡Œï¼Œè€—æ—¶ {} æ¯«ç§’", lineCount, endTime - startTime);
            
        } catch (Exception e) {
            logger.error("æµå¼è¯»å–å¤§æ–‡ä»¶å¼‚å¸¸", e);
        } finally {
            IOUtils.closeStream(reader);
            IOUtils.closeStream(in);
        }
    }
    
    /**
     * åˆ†å—è¯»å–å¤§æ–‡ä»¶
     * 
     * @param fs æ–‡ä»¶ç³»ç»Ÿå¯¹è±¡
     */
    private static void blockReadLargeFile(FileSystem fs) {
        FSDataInputStream in = null;
        
        try {
            Path largePath = new Path("/user/bigdata/large_file.txt");
            
            if (!fs.exists(largePath)) {
                logger.error("å¤§æ–‡ä»¶ä¸å­˜åœ¨: {}", largePath);
                return;
            }
            
            FileStatus fileStatus = fs.getFileStatus(largePath);
            long fileSize = fileStatus.getLen();
            logger.info("æ–‡ä»¶å¤§å°: {} å­—èŠ‚", fileSize);
            
            in = fs.open(largePath);
            
            // åˆ†å—è¯»å–ï¼Œæ¯å—1MB
            int bufferSize = 1024 * 1024; // 1MB
            byte[] buffer = new byte[bufferSize];
            long totalRead = 0;
            int blockCount = 0;
            
            long startTime = System.currentTimeMillis();
            
            while (totalRead < fileSize) {
                int bytesRead = in.read(buffer);
                if (bytesRead == -1) {
                    break;
                }
                
                totalRead += bytesRead;
                blockCount++;
                
                // å¤„ç†è¯»å–çš„æ•°æ®å—
                // processBlock(buffer, bytesRead);
                
                logger.info("å·²è¯»å–å— {}, å¤§å° {} å­—èŠ‚, æ€»è¿›åº¦ {}/{} å­—èŠ‚", 
                           blockCount, bytesRead, totalRead, fileSize);
            }
            
            long endTime = System.currentTimeMillis();
            logger.info("åˆ†å—è¯»å–å®Œæˆï¼Œæ€»å…±è¯»å– {} ä¸ªå—ï¼Œè€—æ—¶ {} æ¯«ç§’", blockCount, endTime - startTime);
            
        } catch (Exception e) {
            logger.error("åˆ†å—è¯»å–å¤§æ–‡ä»¶å¼‚å¸¸", e);
        } finally {
            IOUtils.closeStream(in);
        }
    }
}
```

## 7. å®Œæ•´é¡¹ç›®æ¡ˆä¾‹

### 7.1 æ—¥å¿—æ–‡ä»¶åˆ†æå™¨
```java
package com.bigdata.hdfs.project;

import com.bigdata.hdfs.util.HDFSUtil;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.io.BufferedReader;
import java.io.StringReader;
import java.util.HashMap;
import java.util.Map;
import java.util.regex.Matcher;
import java.util.regex.Pattern;

/**
 * æ—¥å¿—æ–‡ä»¶åˆ†æå™¨
 * åˆ†æHDFSä¸­çš„WebæœåŠ¡å™¨æ—¥å¿—æ–‡ä»¶
 * 
 * @author BigData Team
 * @version 1.0.0
 */
public class LogAnalyzer {
    
    private static final Logger logger = LoggerFactory.getLogger(LogAnalyzer.class);
    
    // Apacheæ—¥å¿—æ ¼å¼æ­£åˆ™è¡¨è¾¾å¼
    private static final String LOG_PATTERN = 
        "^(\\S+) \\S+ \\S+ \\[([\\w:/]+\\s[+\\-]\\d{4})\\] \"(\\S+) (\\S+) (\\S+)\" (\\d{3}) (\\d+)";
    
    private static final Pattern pattern = Pattern.compile(LOG_PATTERN);
    
    public static void main(String[] args) {
        LogAnalyzer analyzer = new LogAnalyzer();
        
        try {
            // 1. åˆ›å»ºæµ‹è¯•æ—¥å¿—æ–‡ä»¶
            analyzer.createTestLogFile();
            
            // 2. åˆ†ææ—¥å¿—æ–‡ä»¶
            analyzer.analyzeLogFile("/user/bigdata/access.log");
            
        } catch (Exception e) {
            logger.error("æ—¥å¿—åˆ†æå¼‚å¸¸", e);
        }
    }
    
    /**
     * åˆ›å»ºæµ‹è¯•æ—¥å¿—æ–‡ä»¶
     */
    private void createTestLogFile() {
        try {
            HDFSUtil hdfsUtil = new HDFSUtil("hdfs://10.132.144.24:9000");
            
            // åˆ›å»ºæ¨¡æ‹Ÿçš„Apacheè®¿é—®æ—¥å¿—
            StringBuilder logContent = new StringBuilder();
            String[] ips = {"192.168.1.100", "192.168.1.101", "192.168.1.102", "10.0.0.1", "10.0.0.2"};
            String[] methods = {"GET", "POST", "PUT", "DELETE"};
            String[] urls = {"/index.html", "/api/users", "/api/orders", "/images/logo.png", "/css/style.css"};
            int[] statusCodes = {200, 404, 500, 301, 403};
            
            for (int i = 0; i < 1000; i++) {
                String ip = ips[i % ips.length];
                String method = methods[i % methods.length];
                String url = urls[i % urls.length];
                int statusCode = statusCodes[i % statusCodes.length];
                int responseSize = (int) (Math.random() * 10000) + 100;
                
                String logLine = String.format(
                    "%s - - [25/Dec/2023:10:%02d:%02d +0800] \"%s %s HTTP/1.1\" %d %d\n",
                    ip, i / 60, i % 60, method, url, statusCode, responseSize
                );
                
                logContent.append(logLine);
            }
            
            // ä¸Šä¼ åˆ°HDFS
            hdfsUtil.writeFile("/user/bigdata/access.log", logContent.toString());
            logger.info("æµ‹è¯•æ—¥å¿—æ–‡ä»¶åˆ›å»ºå®Œæˆ");
            
            hdfsUtil.close();
            
        } catch (Exception e) {
            logger.error("åˆ›å»ºæµ‹è¯•æ—¥å¿—æ–‡ä»¶å¼‚å¸¸", e);
        }
    }
    
    /**
     * åˆ†ææ—¥å¿—æ–‡ä»¶
     * 
     * @param logFilePath æ—¥å¿—æ–‡ä»¶è·¯å¾„
     */
    private void analyzeLogFile(String logFilePath) {
        try {
            HDFSUtil hdfsUtil = new HDFSUtil("hdfs://10.132.144.24:9000");
            
            // è¯»å–æ—¥å¿—æ–‡ä»¶å†…å®¹
            String logContent = hdfsUtil.readFile(logFilePath);
            if (logContent == null) {
                logger.error("æ— æ³•è¯»å–æ—¥å¿—æ–‡ä»¶: {}", logFilePath);
                return;
            }
            
            // åˆ†æç»Ÿè®¡
            Map<String, Integer> ipCount = new HashMap<>();
            Map<String, Integer> statusCount = new HashMap<>();
            Map<String, Integer> methodCount = new HashMap<>();
            long totalBytes = 0;
            int totalRequests = 0;
            
            BufferedReader reader = new BufferedReader(new StringReader(logContent));
            String line;
            
            while ((line = reader.readLine()) != null) {
                Matcher matcher = pattern.matcher(line);
                
                if (matcher.matches()) {
                    String ip = matcher.group(1);
                    String method = matcher.group(3);
                    String statusCode = matcher.group(6);
                    String responseSize = matcher.group(7);
                    
                    // ç»Ÿè®¡IPè®¿é—®æ¬¡æ•°
                    ipCount.put(ip, ipCount.getOrDefault(ip, 0) + 1);
                    
                    // ç»Ÿè®¡çŠ¶æ€ç 
                    statusCount.put(statusCode, statusCount.getOrDefault(statusCode, 0) + 1);
                    
                    // ç»Ÿè®¡è¯·æ±‚æ–¹æ³•
                    methodCount.put(method, methodCount.getOrDefault(method, 0) + 1);
                    
                    // ç´¯è®¡å“åº”å¤§å°
                    totalBytes += Long.parseLong(responseSize);
                    totalRequests++;
                }
            }
            
            // è¾“å‡ºåˆ†æç»“æœ
            logger.info("=== æ—¥å¿—åˆ†æç»“æœ ===");
            logger.info("æ€»è¯·æ±‚æ•°: {}", totalRequests);
            logger.info("æ€»å“åº”å­—èŠ‚æ•°: {} ({} MB)", totalBytes, totalBytes / 1024 / 1024);
            logger.info("å¹³å‡å“åº”å¤§å°: {} å­—èŠ‚", totalBytes / totalRequests);
            
            logger.info("\n=== IPè®¿é—®ç»Ÿè®¡ ===");
            ipCount.entrySet().stream()
                .sorted(Map.Entry.<String, Integer>comparingByValue().reversed())
                .forEach(entry -> logger.info("{}: {} æ¬¡", entry.getKey(), entry.getValue()));
            
            logger.info("\n=== çŠ¶æ€ç ç»Ÿè®¡ ===");
            statusCount.entrySet().stream()
                .sorted(Map.Entry.<String, Integer>comparingByValue().reversed())
                .forEach(entry -> logger.info("{}: {} æ¬¡", entry.getKey(), entry.getValue()));
            
            logger.info("\n=== è¯·æ±‚æ–¹æ³•ç»Ÿè®¡ ===");
            methodCount.entrySet().stream()
                .sorted(Map.Entry.<String, Integer>comparingByValue().reversed())
                .forEach(entry -> logger.info("{}: {} æ¬¡", entry.getKey(), entry.getValue()));
            
            // ä¿å­˜åˆ†æç»“æœåˆ°HDFS
            saveAnalysisResult(hdfsUtil, ipCount, statusCount, methodCount, totalRequests, totalBytes);
            
            hdfsUtil.close();
            
        } catch (Exception e) {
            logger.error("åˆ†ææ—¥å¿—æ–‡ä»¶å¼‚å¸¸", e);
        }
    }
    
    /**
     * ä¿å­˜åˆ†æç»“æœåˆ°HDFS
     */
    private void saveAnalysisResult(HDFSUtil hdfsUtil, Map<String, Integer> ipCount, 
                                   Map<String, Integer> statusCount, Map<String, Integer> methodCount,
                                   int totalRequests, long totalBytes) {
        try {
            StringBuilder result = new StringBuilder();
            result.append("=== æ—¥å¿—åˆ†ææŠ¥å‘Š ===\n");
            result.append("åˆ†ææ—¶é—´: ").append(new java.util.Date()).append("\n\n");
            
            result.append("æ€»ä½“ç»Ÿè®¡:\n");
            result.append("æ€»è¯·æ±‚æ•°: ").append(totalRequests).append("\n");
            result.append("æ€»å“åº”å­—èŠ‚æ•°: ").append(totalBytes).append("\n");
            result.append("å¹³å‡å“åº”å¤§å°: ").append(totalBytes / totalRequests).append(" å­—èŠ‚\n\n");
            
            result.append("IPè®¿é—®æ’è¡Œ:\n");
            ipCount.entrySet().stream()
                .sorted(Map.Entry.<String, Integer>comparingByValue().reversed())
                .limit(10)
                .forEach(entry -> result.append(entry.getKey()).append(": ").append(entry.getValue()).append(" æ¬¡\n"));
            
            result.append("\nçŠ¶æ€ç ç»Ÿè®¡:\n");
            statusCount.entrySet().stream()
                .sorted(Map.Entry.<String, Integer>comparingByValue().reversed())
                .forEach(entry -> result.append(entry.getKey()).append(": ").append(entry.getValue()).append(" æ¬¡\n"));
            
            // ä¿å­˜åˆ°HDFS
            hdfsUtil.writeFile("/user/bigdata/log_analysis_report.txt", result.toString());
            logger.info("åˆ†æç»“æœå·²ä¿å­˜åˆ°: /user/bigdata/log_analysis_report.txt");
            
        } catch (Exception e) {
            logger.error("ä¿å­˜åˆ†æç»“æœå¼‚å¸¸", e);
        }
    }
}
```

## 8. å¸¸è§é—®é¢˜è§£å†³

### 8.1 æƒé™é—®é¢˜
```java
// è®¾ç½®ç”¨æˆ·å
System.setProperty("HADOOP_USER_NAME", "hadoop");

// æˆ–è€…åœ¨Configurationä¸­è®¾ç½®
conf.set("hadoop.security.authentication", "simple");
conf.set("hadoop.security.authorization", "false");
```

### 8.2 è¿æ¥é—®é¢˜
```java
// æ£€æŸ¥HDFSæœåŠ¡æ˜¯å¦å¯åŠ¨
// ç¡®ä¿é˜²ç«å¢™è®¾ç½®æ­£ç¡®
// éªŒè¯é…ç½®æ–‡ä»¶ä¸­çš„åœ°å€å’Œç«¯å£

// æ·»åŠ è¿æ¥è¶…æ—¶è®¾ç½®
conf.set("ipc.client.connect.timeout", "10000");
conf.set("ipc.client.connect.max.retries", "3");
```

### 8.3 å†…å­˜é—®é¢˜
```java
// å¤„ç†å¤§æ–‡ä»¶æ—¶è®¾ç½®åˆé€‚çš„ç¼“å†²åŒºå¤§å°
conf.set("io.file.buffer.size", "131072"); // 128KB

// JVMå‚æ•°è®¾ç½®
// -Xmx2g -Xms1g
```

### 8.4 ç¼–ç é—®é¢˜
```java
// ç»Ÿä¸€ä½¿ç”¨UTF-8ç¼–ç 
String content = new String(bytes, "UTF-8");
outputStream.write(content.getBytes("UTF-8"));
```

## æ€»ç»“

é€šè¿‡æœ¬æ•™ç¨‹ï¼Œæ‚¨å·²ç»å­¦ä¼šäº†ï¼š

1. **ç¯å¢ƒé…ç½®**: Mavené¡¹ç›®é…ç½®å’Œä¾èµ–ç®¡ç†
2. **åŸºç¡€æ“ä½œ**: æ–‡ä»¶å’Œç›®å½•çš„åˆ›å»ºã€åˆ é™¤ã€ä¸Šä¼ ã€ä¸‹è½½
3. **é«˜çº§æ“ä½œ**: å¤§æ–‡ä»¶å¤„ç†ã€æµå¼è¯»å†™
4. **å®é™…åº”ç”¨**: æ—¥å¿—æ–‡ä»¶åˆ†æé¡¹ç›®
5. **é—®é¢˜è§£å†³**: å¸¸è§é—®é¢˜çš„è§£å†³æ–¹æ¡ˆ

**ä¸‹ä¸€æ­¥å­¦ä¹ å»ºè®®ï¼š**
- æ·±å…¥å­¦ä¹ MapReduceç¼–ç¨‹
- äº†è§£HDFSçš„å†…éƒ¨æœºåˆ¶
- å­¦ä¹ Hadoopé›†ç¾¤ç®¡ç†
- æ¢ç´¢å…¶ä»–Hadoopç”Ÿæ€ç»„ä»¶

**ç»ƒä¹ å»ºè®®ï¼š**
1. å®ç°ä¸€ä¸ªæ–‡ä»¶å¤‡ä»½å·¥å…·
2. å¼€å‘ä¸€ä¸ªæ—¥å¿—æ”¶é›†ç³»ç»Ÿ
3. åˆ›å»ºä¸€ä¸ªæ•°æ®è¿ç§»å·¥å…·
4. æ„å»ºä¸€ä¸ªæ–‡ä»¶ç›‘æ§ç³»ç»Ÿ

ç¥æ‚¨å­¦ä¹ æ„‰å¿«ï¼ğŸš€